# -*- coding: utf-8 -*-
"""Projet_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15afnIuzc5ManI0N_AjJXE1_9wWrHkhpX
"""

# Cellule 1: Installation des bibliothèques
# Exécutez cette cellule une seule fois par session.
!pip install pandas numpy matplotlib seaborn scikit-learn -q

# --- Importations des modules nécessaires (vous avez dit les avoir déjà faites) ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm # Pour des statistiques de régression plus détaillées

# --- 1. Chargement de la base de données depuis Google Drive ---
# 'upload' est la variable contenant le chemin vers votre fichier sur Google Drive
# Exemple de ce que 'upload' pourrait contenir si vous avez monté votre Drive :
# upload = '/content/drive/MyDrive/chemin_vers_votre_dossier/dataset.csv'

# Si vous avez utilisé files.upload() et 'upload' est un dictionnaire :
# from google.colab import files
# import io
# uploaded_files = files.upload() # Vous l'avez peut-être appelé 'upload' directement
# nom_fichier = list(uploaded_files.keys())[0]
# df = pd.read_csv(io.BytesIO(uploaded_files[nom_fichier]))

# SI 'upload' EST BIEN LE CHEMIN STRING:
try:
    # Remplacez ceci par la manière dont vous avez réellement défini 'upload'
    # Si 'upload' est déjà le dataframe (par ex. si vous avez fait df = pd.read_csv(...upload...)),
    # alors vous pouvez sauter cette étape et assigner df directement.
    # Pour cet exemple, je suppose que 'upload' est le chemin du fichier.
    df = pd.read_csv(upload) # 'upload' est votre variable contenant le chemin
    print("--- 1. Chargement des données réussi ---")
    print(f"La base de données contient {df.shape[0]} lignes et {df.shape[1]} colonnes.\n")
except FileNotFoundError:
    print(f"Erreur : Le fichier à l'emplacement '{upload}' n'a pas été trouvé.")
    print("Veuillez vérifier que la variable 'upload' contient le chemin correct et que le fichier existe.")
    print("Si vous avez monté Google Drive, le chemin commence souvent par '/content/drive/MyDrive/'.")
    exit()
except Exception as e:
    print(f"Une erreur est survenue lors du chargement du fichier : {e}")
    print("Assurez-vous que la variable 'upload' est correctement définie.")
    exit()

# --- 2. Exploration Initiale des Données ---
print("--- 2. Exploration Initiale des Données ---")
print("\n--- Aperçu des 5 premières lignes ---")
print(df.head())

print("\n--- Informations sur les colonnes (types, valeurs non nulles) ---")
df.info()

print("\n--- Statistiques descriptives pour les colonnes numériques ---")
print(df.describe(include='all')) # include='all' pour voir aussi les stats des colonnes non-numériques

print("\n--- Nombre de valeurs manquantes par colonne ---")
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])
if missing_values.sum() == 0:
    print("Aucune valeur manquante détectée.")

# --- 3. Nettoyage et Prétraitement des Données (À ADAPTER) ---
print("\n--- 3. Nettoyage et Prétraitement (Exemples à adapter) ---")

# Créez une copie pour travailler dessus pour éviter de modifier l'original df directement
df_processed = df.copy()

# Exemple : Gestion des valeurs manquantes (à adapter selon vos besoins)
# Option 1: Supprimer les lignes avec des NaN (simple mais peut perdre des données)
# df_processed.dropna(inplace=True) # À utiliser avec précaution

# Option 2: Imputer les valeurs manquantes
# Pour les colonnes numériques, on peut utiliser la moyenne ou la médiane
numerical_cols_with_nan = df_processed.select_dtypes(include=np.number).isnull().any()
cols_to_impute_mean = numerical_cols_with_nan[numerical_cols_with_nan].index
for col in cols_to_impute_mean:
    if df_processed[col].isnull().sum() > 0: # Vérifier à nouveau au cas où
        print(f"Imputation de la moyenne pour les NaN dans la colonne numérique '{col}'")
        df_processed[col].fillna(df_processed[col].mean(), inplace=True)

# Pour les colonnes catégorielles, on peut utiliser le mode ou une catégorie 'Inconnu'
categorical_cols_with_nan = df_processed.select_dtypes(include='object').isnull().any()
cols_to_impute_mode = categorical_cols_with_nan[categorical_cols_with_nan].index
for col in cols_to_impute_mode:
    if df_processed[col].isnull().sum() > 0: # Vérifier à nouveau
        print(f"Imputation du mode pour les NaN dans la colonne catégorielle '{col}'")
        df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)

# Exemple : Conversion de types de données si nécessaire
# if 'ma_colonne_numérique_en_texte' in df_processed.columns:
#     df_processed['ma_colonne_numérique_en_texte'] = pd.to_numeric(df_processed['ma_colonne_numérique_en_texte'], errors='coerce')

# Exemple : Suppression des doublons
initial_rows = len(df_processed)
df_processed.drop_duplicates(inplace=True)
if initial_rows > len(df_processed):
    print(f"{initial_rows - len(df_processed)} lignes dupliquées ont été supprimées.")
else:
    print("Aucune ligne dupliquée trouvée.")

print("Le nettoyage et prétraitement de base est terminé. Vérifiez et adaptez ces étapes.")

# --- 4. Analyse Exploratoire des Données (EDA) avec Visualisations ---
print("\n--- 4. Analyse Exploratoire des Données (Visualisations) ---")

numerical_cols = df_processed.select_dtypes(include=np.number).columns.tolist()
categorical_cols = df_processed.select_dtypes(include='object').columns.tolist()

print(f"Colonnes numériques identifiées : {numerical_cols}")
print(f"Colonnes catégorielles identifiées : {categorical_cols}")

if numerical_cols:
    print("\n--- Distribution des variables numériques ---")
    for col in numerical_cols:
        plt.figure(figsize=(8, 5))
        sns.histplot(df_processed[col].dropna(), kde=True, bins=30) # dropna() pour éviter erreurs si imputation non faite
        plt.title(f'Distribution de {col}')
        plt.xlabel(col)
        plt.ylabel('Fréquence')
        plt.show() # Important dans Colab pour afficher le graphique
else:
    print("Aucune colonne numérique trouvée pour les histogrammes.")

if len(numerical_cols) > 1:
    print("\n--- Matrice de corrélation des variables numériques ---")
    correlation_matrix = df_processed[numerical_cols].corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
    plt.title('Matrice de Corrélation')
    plt.show() # Important dans Colab
    print("Inspectez cette matrice pour identifier des paires de variables potentiellement corrélées.")
else:
    print("Pas assez de colonnes numériques (>1) pour calculer une matrice de corrélation.")

# --- 5. Préparation pour la Régression Linéaire ---
print("\n--- 5. Préparation pour la Régression Linéaire ---")
print("Choisissez une variable cible (Y) et une ou plusieurs variables explicatives (X).")

# !!! CHOISISSEZ VOS VARIABLES ICI !!!
# Remplacez 'nom_variable_cible' et les éléments de 'liste_variables_explicatives'
# par les vrais noms de colonnes de votre dataset (df_processed).
# Assurez-vous qu'elles sont numériques et qu'elles n'ont pas trop de valeurs manquantes
# (ou qu'elles ont été traitées à l'étape 3).

target_variable = None    # EXEMPLE: 'popularity' (à remplacer par un vrai nom de colonne numérique)
feature_variables = []  # EXEMPLE: ['danceability', 'energy'] (à remplacer)

# --- EXEMPLE DE SÉLECTION (DÉCOMMENTEZ ET ADAPTEZ) ---
# print("\nColonnes numériques disponibles pour la régression :", numerical_cols)
# target_variable = 'popularity' # Mettez ici le nom de votre variable cible (ex: 'Stream')
# feature_variables = ['danceability', 'energy', 'valence'] # Mettez ici les noms de vos variables explicatives

# Vérification que les variables choisies existent et sont numériques
valid_selection = False
if target_variable and feature_variables:
    if target_variable in df_processed.columns and df_processed[target_variable].dtype in [np.int64, np.float64, int, float]:
        valid_target = True
    else:
        print(f"Erreur : La variable cible '{target_variable}' n'existe pas dans df_processed ou n'est pas numérique.")
        valid_target = False

    valid_features = True
    for feature in feature_variables:
        if feature not in df_processed.columns or df_processed[feature].dtype not in [np.int64, np.float64, int, float]:
            print(f"Erreur : La variable explicative '{feature}' n'existe pas dans df_processed ou n'est pas numérique.")
            valid_features = False
            break
    valid_selection = valid_target and valid_features
else:
    print("\nATTENTION : Vous n'avez pas encore défini 'target_variable' et 'feature_variables'.")
    print("Veuillez les définir dans le code (juste au-dessus de cette section) pour exécuter la régression.")

if valid_selection:
    print(f"\nAnalyse de régression pour prédire '{target_variable}' à partir de {feature_variables}.")

    # Création des ensembles X et y à partir de df_processed
    # S'assurer que les colonnes sélectionnées n'ont plus de NaN
    # On crée un sous-ensemble temporaire pour faciliter la suppression des NaN uniquement pour la régression
    cols_for_regression = [target_variable] + feature_variables
    df_regression_subset = df_processed[cols_for_regression].copy()

    rows_before_dropna = len(df_regression_subset)
    df_regression_subset.dropna(inplace=True) # Supprime les lignes où AU MOINS UNE des colonnes sélectionnées est NaN
    rows_after_dropna = len(df_regression_subset)

    if rows_before_dropna > rows_after_dropna:
        print(f"{rows_before_dropna - rows_after_dropna} lignes contenant des NaN dans les variables sélectionnées pour la régression ont été supprimées.")

    if df_regression_subset.empty:
        print("ERREUR: Plus aucune donnée après suppression des NaN pour les variables de régression sélectionnées. Vérifiez le nettoyage des données ou vos choix de variables.")
    else:
        X = df_regression_subset[feature_variables]
        y = df_regression_subset[target_variable]

        if X.empty or y.empty:
            print("ERREUR: X ou y est vide. Cela peut arriver si toutes les lignes avaient des NaN pour les variables choisies.")
        else:
            # Division des données en ensembles d'entraînement et de test
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            print(f"Taille de l'ensemble d'entraînement : {X_train.shape[0]} échantillons")
            print(f"Taille de l'ensemble de test : {X_test.shape[0]} échantillons")

            if X_train.empty or X_test.empty:
                print("ERREUR: L'ensemble d'entraînement ou de test est vide. Cela peut arriver avec très peu de données après le split.")
            else:
                # --- 6. Entraînement du Modèle de Régression Linéaire (scikit-learn) ---
                print("\n--- 6. Entraînement du Modèle de Régression Linéaire (scikit-learn) ---")
                model_sklearn = LinearRegression()
                model_sklearn.fit(X_train, y_train)

                print(f"Coefficients (pentes) : {model_sklearn.coef_}")
                for feature, coef in zip(feature_variables, model_sklearn.coef_):
                    print(f"  - {feature}: {coef:.4f}")
                print(f"Intercept (ordonnée à l'origine) : {model_sklearn.intercept_:.4f}")

                # --- 7. Évaluation du Modèle ---
                print("\n--- 7. Évaluation du Modèle ---")
                y_pred_test = model_sklearn.predict(X_test)
                y_pred_train = model_sklearn.predict(X_train)

                mse_test = mean_squared_error(y_test, y_pred_test)
                r2_test = r2_score(y_test, y_pred_test)
                mse_train = mean_squared_error(y_train, y_pred_train)
                r2_train = r2_score(y_train, y_pred_train)

                print(f"Sur l'ensemble d'entraînement :")
                print(f"  Mean Squared Error (MSE) : {mse_train:.2f}")
                print(f"  R-squared (R²) : {r2_train:.2f}")
                print(f"Sur l'ensemble de test :")
                print(f"  Mean Squared Error (MSE) : {mse_test:.2f}")
                print(f"  R-squared (R²) : {r2_test:.2f} (plus c'est proche de 1, mieux c'est)")

                # --- 8. Analyse plus détaillée avec Statsmodels (p-values, etc.) ---
                print("\n--- 8. Analyse plus détaillée avec Statsmodels ---")
                X_train_sm = sm.add_constant(X_train) # Ajoute une colonne de 1s pour l'intercept
                model_sm = sm.OLS(y_train, X_train_sm).fit()
                print(model_sm.summary())
                print("\nInterprétation du résumé de statsmodels :")
                print("  - 'coef' : Les coefficients.")
                print("  - 'P>|t|' (p-value) : Si < 0.05, la variable est statistiquement significative.")
                print("  - 'R-squared' / 'Adj. R-squared' : Proportion de variance expliquée.")

                # Visualisation des résidus
                residuals = y_test - y_pred_test
                plt.figure(figsize=(10, 6))
                sns.scatterplot(x=y_pred_test, y=residuals)
                plt.axhline(0, color='red', linestyle='--')
                plt.xlabel('Valeurs Prédites')
                plt.ylabel('Résidus')
                plt.title('Graphique des Résidus vs Valeurs Prédites')
                plt.show() # Important dans Colab

                # Visualisation pour régression simple (si une seule variable explicative)
                if len(feature_variables) == 1:
                    plt.figure(figsize=(10, 6))
                    # Utiliser X_test et y_test pour la visualisation
                    feature_name = feature_variables[0]
                    plt.scatter(X_test[feature_name], y_test, color='blue', label='Données réelles (Test)')

                    # Pour tracer la ligne, il faut trier les X_test pour que la ligne soit correcte
                    X_test_sorted_indices = X_test[feature_name].sort_values().index
                    X_test_sorted = X_test.loc[X_test_sorted_indices]
                    y_pred_line = model_sklearn.predict(X_test_sorted) # Prédire sur X_test trié

                    plt.plot(X_test_sorted[feature_name], y_pred_line, color='red', linewidth=2, label='Ligne de régression')
                    plt.xlabel(feature_name)
                    plt.ylabel(target_variable)
                    plt.title(f'Régression Linéaire Simple: {target_variable} vs {feature_name}')
                    plt.legend()
                    plt.show() # Important dans Colab
else:
    print("\nLa section de régression linéaire n'a pas été exécutée.")
    if not (target_variable and feature_variables):
        print("Cause : 'target_variable' et/ou 'feature_variables' ne sont pas définies.")
    else:
        print("Cause : Problème avec la sélection des variables (non numériques ou non trouvées). Vérifiez les messages d'erreur ci-dessus.")

print("\n--- Fin du script ---")
print("N'oubliez pas d'analyser les sorties, d'adapter le nettoyage, et d'expérimenter avec différentes variables pour la régression !")

# Cellule 2: Importation des bibliothèques
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm # Pour des statistiques de régression plus détaillées

from google.colab import files # Pour l'upload de fichiers
import io # Pour lire le fichier uploadé
from google.colab import drive

from google.colab import files
import io

uploaded_files_dict = files.upload() # Ceci va ouvrir une boîte de dialogue pour choisir le fichier

if uploaded_files_dict:
    nom_du_fichier_importe = list(uploaded_files_dict.keys())[0]
    # Maintenant, chargez le dataframe
    try:
        df = pd.read_csv(io.BytesIO(uploaded_files_dict[nom_du_fichier_importe]))
        print("--- 1. Chargement des données réussi (via files.upload) ---")
        print(f"La base de données contient {df.shape[0]} lignes et {df.shape[1]} colonnes.\n")
        # Puis vous pouvez commenter ou supprimer la section de chargement avec la variable 'upload'
    except Exception as e:
        print(f"Erreur lors de la lecture du fichier uploadé : {e}")
        exit()
else:
    print("Aucun fichier n'a été uploadé.")
    exit()

drive.mount('/content/drive')

# Cellule 3: Configuration des variables principales
# MODIFIEZ CES VARIABLES SELON VOTRE BASE DE DONNÉES
# Après avoir uploadé votre fichier, mettez son nom exact ici.
UPLOADED_FILE_NAME = '/content/drive/MyDrive/Études/python/Projet_data/dataset.csv' # <--- EXEMPLE: 'ventes_produits.csv'

# Nom de la colonne que vous voulez prédire (variable dépendante/cible).
TARGET_VARIABLE = 'popularity' # <--- EXEMPLE: 'chiffre_affaires'

# Liste des colonnes que vous voulez utiliser comme variables explicatives (indépendantes).
# Laissez None pour utiliser toutes les colonnes numériques sauf la cible (après nettoyage).
# EXEMPLE: INITIAL_FEATURES = ['publicite_tv', 'prix_unitaire', 'nombre_concurrents']
INITIAL_FEATURES = None

# Pour la reproductibilité des résultats
RANDOM_STATE = 42

# Cellule 4: Upload du fichier de données
# Exécutez cette cellule, puis cliquez sur "Choisir les fichiers" pour uploader votre CSV.
upload = UPLOADED_FILE_NAME

#nom_fichier = list(uploaded_files.keys())[0]
#df = pd.read_csv(io.BytesIO(upload))

# SI 'upload' EST BIEN LE CHEMIN STRING:
try:
    # Remplacez ceci par la manière dont vous avez réellement défini 'upload'
    # Si 'upload' est déjà le dataframe (par ex. si vous avez fait df = pd.read_csv(...upload...)),
    # alors vous pouvez sauter cette étape et assigner df directement.
    # Pour cet exemple, je suppose que 'upload' est le chemin du fichier.
    df = pd.read_csv(upload) # 'upload' est votre variable contenant le chemin
    print("--- 1. Chargement des données réussi ---")
    print(f"La base de données contient {df.shape[0]} lignes et {df.shape[1]} colonnes.\n")
except FileNotFoundError:
    print(f"Erreur : Le fichier à l'emplacement '{upload}' n'a pas été trouvé.")
    print("Veuillez vérifier que la variable 'upload' contient le chemin correct et que le fichier existe.")
    print("Si vous avez monté Google Drive, le chemin commence souvent par '/content/drive/MyDrive/'.")
    exit()
except Exception as e:
    print(f"Une erreur est survenue lors du chargement du fichier : {e}")
    print("Assurez-vous que la variable 'upload' est correctement définie.")
    exit()

# --- 2. Exploration Initiale des Données ---
print("--- 2. Exploration Initiale des Données ---")
print("\n--- Aperçu des 5 premières lignes ---")
print(df.head())

print("\n--- Informations sur les colonnes (types, valeurs non nulles) ---")
df.info()

print("\n--- Statistiques descriptives pour les colonnes numériques ---")
print(df.describe(include='all')) # include='all' pour voir aussi les stats des colonnes non-numériques

print("\n--- Nombre de valeurs manquantes par colonne ---")
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])
if missing_values.sum() == 0:
    print("Aucune valeur manquante détectée.")

# --- 3. Nettoyage et Prétraitement des Données (À ADAPTER) ---
print("\n--- 3. Nettoyage et Prétraitement (Exemples à adapter) ---")

# Créez une copie pour travailler dessus pour éviter de modifier l'original df directement
df_processed = df.copy()

# Exemple : Gestion des valeurs manquantes (à adapter selon vos besoins)
# Option 1: Supprimer les lignes avec des NaN (simple mais peut perdre des données)
# df_processed.dropna(inplace=True) # À utiliser avec précaution

# Option 2: Imputer les valeurs manquantes
# Pour les colonnes numériques, on peut utiliser la moyenne ou la médiane
numerical_cols_with_nan = df_processed.select_dtypes(include=np.number).isnull().any()
cols_to_impute_mean = numerical_cols_with_nan[numerical_cols_with_nan].index
for col in cols_to_impute_mean:
    if df_processed[col].isnull().sum() > 0: # Vérifier à nouveau au cas où
        print(f"Imputation de la moyenne pour les NaN dans la colonne numérique '{col}'")
        df_processed[col].fillna(df_processed[col].mean(), inplace=True)

# Pour les colonnes catégorielles, on peut utiliser le mode ou une catégorie 'Inconnu'
categorical_cols_with_nan = df_processed.select_dtypes(include='object').isnull().any()
cols_to_impute_mode = categorical_cols_with_nan[categorical_cols_with_nan].index
for col in cols_to_impute_mode:
    if df_processed[col].isnull().sum() > 0: # Vérifier à nouveau
        print(f"Imputation du mode pour les NaN dans la colonne catégorielle '{col}'")
        df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)

# Exemple : Conversion de types de données si nécessaire
# if 'ma_colonne_numérique_en_texte' in df_processed.columns:
#     df_processed['ma_colonne_numérique_en_texte'] = pd.to_numeric(df_processed['ma_colonne_numérique_en_texte'], errors='coerce')

# Exemple : Suppression des doublons
initial_rows = len(df_processed)
df_processed.drop_duplicates(inplace=True)
if initial_rows > len(df_processed):
    print(f"{initial_rows - len(df_processed)} lignes dupliquées ont été supprimées.")
else:
    print("Aucune ligne dupliquée trouvée.")

print("Le nettoyage et prétraitement de base est terminé. Vérifiez et adaptez ces étapes.")

# --- 4. Analyse Exploratoire des Données (EDA) avec Visualisations ---
print("\n--- 4. Analyse Exploratoire des Données (Visualisations) ---")

numerical_cols = df_processed.select_dtypes(include=np.number).columns.tolist()
categorical_cols = df_processed.select_dtypes(include='object').columns.tolist()

print(f"Colonnes numériques identifiées : {numerical_cols}")
print(f"Colonnes catégorielles identifiées : {categorical_cols}")

if numerical_cols:
    print("\n--- Distribution des variables numériques ---")
    for col in numerical_cols:
        plt.figure(figsize=(8, 5))
        sns.histplot(df_processed[col].dropna(), kde=True, bins=30) # dropna() pour éviter erreurs si imputation non faite
        plt.title(f'Distribution de {col}')
        plt.xlabel(col)
        plt.ylabel('Fréquence')
        plt.show() # Important dans Colab pour afficher le graphique
else:
    print("Aucune colonne numérique trouvée pour les histogrammes.")

if len(numerical_cols) > 1:
    print("\n--- Matrice de corrélation des variables numériques ---")
    correlation_matrix = df_processed[numerical_cols].corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
    plt.title('Matrice de Corrélation')
    plt.show() # Important dans Colab
    print("Inspectez cette matrice pour identifier des paires de variables potentiellement corrélées.")
else:
    print("Pas assez de colonnes numériques (>1) pour calculer une matrice de corrélation.")

# --- 5. Préparation pour la Régression Linéaire ---
print("\n--- 5. Préparation pour la Régression Linéaire ---")
print("Choisissez une variable cible (Y) et une ou plusieurs variables explicatives (X).")

# !!! CHOISISSEZ VOS VARIABLES ICI !!!
# Remplacez 'nom_variable_cible' et les éléments de 'liste_variables_explicatives'
# par les vrais noms de colonnes de votre dataset (df_processed).
# Assurez-vous qu'elles sont numériques et qu'elles n'ont pas trop de valeurs manquantes
# (ou qu'elles ont été traitées à l'étape 3).

target_variable = None    # EXEMPLE: 'popularity' (à remplacer par un vrai nom de colonne numérique)
feature_variables = []  # EXEMPLE: ['danceability', 'energy'] (à remplacer)

# --- EXEMPLE DE SÉLECTION (DÉCOMMENTEZ ET ADAPTEZ) ---
# print("\nColonnes numériques disponibles pour la régression :", numerical_cols)
# target_variable = 'popularity' # Mettez ici le nom de votre variable cible (ex: 'Stream')
# feature_variables = ['danceability', 'energy', 'valence'] # Mettez ici les noms de vos variables explicatives

# Vérification que les variables choisies existent et sont numériques
valid_selection = False
if target_variable and feature_variables:
    if target_variable in df_processed.columns and df_processed[target_variable].dtype in [np.int64, np.float64, int, float]:
        valid_target = True
    else:
        print(f"Erreur : La variable cible '{target_variable}' n'existe pas dans df_processed ou n'est pas numérique.")
        valid_target = False

    valid_features = True
    for feature in feature_variables:
        if feature not in df_processed.columns or df_processed[feature].dtype not in [np.int64, np.float64, int, float]:
            print(f"Erreur : La variable explicative '{feature}' n'existe pas dans df_processed ou n'est pas numérique.")
            valid_features = False
            break
    valid_selection = valid_target and valid_features
else:
    print("\nATTENTION : Vous n'avez pas encore défini 'target_variable' et 'feature_variables'.")
    print("Veuillez les définir dans le code (juste au-dessus de cette section) pour exécuter la régression.")

if valid_selection:
    print(f"\nAnalyse de régression pour prédire '{target_variable}' à partir de {feature_variables}.")

    # Création des ensembles X et y à partir de df_processed
    # S'assurer que les colonnes sélectionnées n'ont plus de NaN
    # On crée un sous-ensemble temporaire pour faciliter la suppression des NaN uniquement pour la régression
    cols_for_regression = [target_variable] + feature_variables
    df_regression_subset = df_processed[cols_for_regression].copy()

    rows_before_dropna = len(df_regression_subset)
    df_regression_subset.dropna(inplace=True) # Supprime les lignes où AU MOINS UNE des colonnes sélectionnées est NaN
    rows_after_dropna = len(df_regression_subset)

    if rows_before_dropna > rows_after_dropna:
        print(f"{rows_before_dropna - rows_after_dropna} lignes contenant des NaN dans les variables sélectionnées pour la régression ont été supprimées.")

    if df_regression_subset.empty:
        print("ERREUR: Plus aucune donnée après suppression des NaN pour les variables de régression sélectionnées. Vérifiez le nettoyage des données ou vos choix de variables.")
    else:
        X = df_regression_subset[feature_variables]
        y = df_regression_subset[target_variable]

        if X.empty or y.empty:
            print("ERREUR: X ou y est vide. Cela peut arriver si toutes les lignes avaient des NaN pour les variables choisies.")
        else:
            # Division des données en ensembles d'entraînement et de test
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            print(f"Taille de l'ensemble d'entraînement : {X_train.shape[0]} échantillons")
            print(f"Taille de l'ensemble de test : {X_test.shape[0]} échantillons")

            if X_train.empty or X_test.empty:
                print("ERREUR: L'ensemble d'entraînement ou de test est vide. Cela peut arriver avec très peu de données après le split.")
            else:
                # --- 6. Entraînement du Modèle de Régression Linéaire (scikit-learn) ---
                print("\n--- 6. Entraînement du Modèle de Régression Linéaire (scikit-learn) ---")
                model_sklearn = LinearRegression()
                model_sklearn.fit(X_train, y_train)

                print(f"Coefficients (pentes) : {model_sklearn.coef_}")
                for feature, coef in zip(feature_variables, model_sklearn.coef_):
                    print(f"  - {feature}: {coef:.4f}")
                print(f"Intercept (ordonnée à l'origine) : {model_sklearn.intercept_:.4f}")

                # --- 7. Évaluation du Modèle ---
                print("\n--- 7. Évaluation du Modèle ---")
                y_pred_test = model_sklearn.predict(X_test)
                y_pred_train = model_sklearn.predict(X_train)

                mse_test = mean_squared_error(y_test, y_pred_test)
                r2_test = r2_score(y_test, y_pred_test)
                mse_train = mean_squared_error(y_train, y_pred_train)
                r2_train = r2_score(y_train, y_pred_train)

                print(f"Sur l'ensemble d'entraînement :")
                print(f"  Mean Squared Error (MSE) : {mse_train:.2f}")
                print(f"  R-squared (R²) : {r2_train:.2f}")
                print(f"Sur l'ensemble de test :")
                print(f"  Mean Squared Error (MSE) : {mse_test:.2f}")
                print(f"  R-squared (R²) : {r2_test:.2f} (plus c'est proche de 1, mieux c'est)")

                # --- 8. Analyse plus détaillée avec Statsmodels (p-values, etc.) ---
                print("\n--- 8. Analyse plus détaillée avec Statsmodels ---")
                X_train_sm = sm.add_constant(X_train) # Ajoute une colonne de 1s pour l'intercept
                model_sm = sm.OLS(y_train, X_train_sm).fit()
                print(model_sm.summary())
                print("\nInterprétation du résumé de statsmodels :")
                print("  - 'coef' : Les coefficients.")
                print("  - 'P>|t|' (p-value) : Si < 0.05, la variable est statistiquement significative.")
                print("  - 'R-squared' / 'Adj. R-squared' : Proportion de variance expliquée.")

                # Visualisation des résidus
                residuals = y_test - y_pred_test
                plt.figure(figsize=(10, 6))
                sns.scatterplot(x=y_pred_test, y=residuals)
                plt.axhline(0, color='red', linestyle='--')
                plt.xlabel('Valeurs Prédites')
                plt.ylabel('Résidus')
                plt.title('Graphique des Résidus vs Valeurs Prédites')
                plt.show() # Important dans Colab

                # Visualisation pour régression simple (si une seule variable explicative)
                if len(feature_variables) == 1:
                    plt.figure(figsize=(10, 6))
                    # Utiliser X_test et y_test pour la visualisation
                    feature_name = feature_variables[0]
                    plt.scatter(X_test[feature_name], y_test, color='blue', label='Données réelles (Test)')

                    # Pour tracer la ligne, il faut trier les X_test pour que la ligne soit correcte
                    X_test_sorted_indices = X_test[feature_name].sort_values().index
                    X_test_sorted = X_test.loc[X_test_sorted_indices]
                    y_pred_line = model_sklearn.predict(X_test_sorted) # Prédire sur X_test trié

                    plt.plot(X_test_sorted[feature_name], y_pred_line, color='red', linewidth=2, label='Ligne de régression')
                    plt.xlabel(feature_name)
                    plt.ylabel(target_variable)
                    plt.title(f'Régression Linéaire Simple: {target_variable} vs {feature_name}')
                    plt.legend()
                    plt.show() # Important dans Colab
else:
    print("\nLa section de régression linéaire n'a pas été exécutée.")
    if not (target_variable and feature_variables):
        print("Cause : 'target_variable' et/ou 'feature_variables' ne sont pas définies.")
    else:
        print("Cause : Problème avec la sélection des variables (non numériques ou non trouvées). Vérifiez les messages d'erreur ci-dessus.")

print("\n--- Fin du script ---")
print("N'oubliez pas d'analyser les sorties, d'adapter le nettoyage, et d'expérimenter avec différentes variables pour la régression !")